{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcUvqzyVsDry5/Cu6xuxGf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/NLP-Assignment/blob/main/Assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What are Vanilla autoencoders\n",
        "\n",
        "The vanilla autoencoder, as proposed by Hinton, consists of only one hidden layer.Learning in the autoencoder consists of developing a compact representation of the input signal at the hidden layer so that the output layer can faithfully reproduce the original input: Autoencoder with a single hidden layer ."
      ],
      "metadata": {
        "id": "z2zGrFhO3msW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are Sparse autoencoders\n",
        "\n",
        "A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer."
      ],
      "metadata": {
        "id": "7JTP9aFp3ur6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What are Denoising autoencoders\n",
        "\n",
        "A Denoising Autoencoder is a modification on the autoencoder to prevent the network learning the identity function. Specifically, if the autoencoder is too big, then it can just learn the data, so the output equals the input, and does not perform any useful representation learning or dimensionality reduction."
      ],
      "metadata": {
        "id": "OE4f5ft530-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What are Convolutional autoencoders\n",
        "\n",
        "Convolutional Autoencoder is a variant of Convolutional Neural Networks that are used as the tools for unsupervised learning of convolution filters. They are generally applied in the task of image reconstruction to minimize reconstruction errors by learning the optimal filters."
      ],
      "metadata": {
        "id": "3qXmerS23-6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are Stacked autoencoders\n",
        "\n",
        "A stacked autoencoder is a neural network consist several layers of sparse autoencoders where output of each hidden layer is connected to the input of the successive hidden layer"
      ],
      "metadata": {
        "id": "nsNwSJJt4GFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain how to generate sentences using LSTM autoencoders\n",
        "\n",
        "In text generation, we try to predict the next character or word of the sequence.So, LSTM can be used to predict the next word. The neural network take sequence of words as input and output will be a matrix of probability for each word from dictionary to be next of given sequence"
      ],
      "metadata": {
        "id": "JNT8WwYO4M-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Explain Extractive summarization\n",
        "\n",
        "Extractive summarization aims at identifying the salient information that is then extracted and grouped together to form a concise summary. Abstractive summary generation rewrites the entire document by building internal semantic representation, and then a summary is created using natural language processing."
      ],
      "metadata": {
        "id": "lJNJkg_O4V8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Explain Abstractive summarization\n",
        "\n",
        "Abstractive Text Summarization is the task of generating a short and concise summary that captures the salient ideas of the source text. The generated summaries potentially contain new phrases and sentences that may not appear in the source text."
      ],
      "metadata": {
        "id": "O3INeNyx4bLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Explain Beam search\n",
        "\n",
        "The beam width bounds the memory required to perform the search. Since a goal state could potentially be pruned, beam search sacrifices completeness (the guarantee that an algorithm will terminate with a solution, if one exists)"
      ],
      "metadata": {
        "id": "sIR8sHO_4gv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Explain Length normalization"
      ],
      "metadata": {
        "id": "5A-ATBqU4oxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Explain Coverage normalization"
      ],
      "metadata": {
        "id": "l_p9lZfL49G-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Explain ROUGE metric evaluation\n",
        "\n",
        "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing"
      ],
      "metadata": {
        "id": "y0w0TpN15LAM"
      }
    }
  ]
}