{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKkMCsuarBsCLu/l3XvVAn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/NLP-Assignment/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Explain the basic architecture of RNN cell.\n",
        "\n",
        "There are two main architectures that are used in almost every application of recurrent neural networks: long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al., 2014). Both of these use every time step to calculate an output and to update the internal state"
      ],
      "metadata": {
        "id": "5N0ypgH6xp46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Explain Backpropagation through time (BPTT)\n",
        "\n",
        "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series. A recurrent neural network is shown one input each timestep and predicts one output. Conceptually, BPTT works by unrolling all input timesteps."
      ],
      "metadata": {
        "id": "Vxffwyhqx2N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Explain Vanishing and exploding gradients\n",
        "\n",
        "Exploding gradient occurs when the derivatives or slope will get larger and larger as we go backward with every layer during backpropagation. This situation is the exact opposite of the vanishing gradients. This problem happens because of weights, not because of the activation function"
      ],
      "metadata": {
        "id": "oMUe1Q_DyAQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Explain Long short-term memory (LSTM)\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more. LSTMs are a complex area of deep learning"
      ],
      "metadata": {
        "id": "oT3ArLMoyKmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Explain Gated recurrent unit (GRU)\n",
        "\n",
        "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate."
      ],
      "metadata": {
        "id": "4wJi_GCDyXHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain Peephole LSTM\n",
        "\n",
        "Peephole connections refer to a modification to the basic LSTM architecture. Surprisingly, LSTM augmented by “peephole connections” from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps"
      ],
      "metadata": {
        "id": "3otn1m9iygtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Bidirectional RNNs\n",
        "\n",
        "Bidirectional recurrent neural networks (RNN) are trained to predict both in the positive and negative time directions simultaneously. They have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model has been difficult"
      ],
      "metadata": {
        "id": "JSS2mjAkynF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Explain the gates of LSTM with equations.\n",
        "\n",
        "A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell."
      ],
      "metadata": {
        "id": "0GMAC0_qyu_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Explain BiLSTM\n",
        "\n",
        "Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction.BiLSTM allows us to exploit future and history context together at once. Recently, BiLSTM has been used intensively for real-world applications, ranging from signalprocessing tasks [16] [14] to text processing tasks"
      ],
      "metadata": {
        "id": "fuTlhNioy2fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Explain BiGRU\n",
        "\n",
        "Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates."
      ],
      "metadata": {
        "id": "zL33ALTKzDQo"
      }
    }
  ]
}