{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLIrFXpR5NfJuN+7MrYl9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/NLP-Assignment/blob/main/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What are Sequence-to-sequence models?\n",
        "\n",
        "A typical sequence to sequence model has two parts â€“ an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. ... This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output."
      ],
      "metadata": {
        "id": "9r8P0Bqo1z0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are the Problem with Vanilla RNNs?\n",
        "\n",
        "RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done."
      ],
      "metadata": {
        "id": "2EHZ4opz2EvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is Gradient clipping?\n",
        "\n",
        "Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks.This type of learning algorithm is designed based on the way neurons function in the human brain."
      ],
      "metadata": {
        "id": "aIeyRUFi2NMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Explain Attention mechanism\n",
        "\n",
        "The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation."
      ],
      "metadata": {
        "id": "O6dq8_4a2VGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Explain Conditional random fields (CRFs)\n",
        "\n",
        "Conditional Random Fields (CRF) CRF is a discriminant model for sequences data similar to MEMM. It models the dependency between each state and the entire input sequences. Unlike MEMM, CRF overcomes the label bias issue by using global normalizer."
      ],
      "metadata": {
        "id": "yB4Juav22bkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain self-attention\n",
        "\n",
        "Self-Attention. The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input"
      ],
      "metadata": {
        "id": "Zx37u1fM2iT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What is Bahdanau Attention?\n",
        "\n",
        "The Bahdanau attention was proposed to address the performance bottleneck of conventional encoder-decoder architectures, achieving significant improvements over the conventional approach.Bahdanau Attention is also known as Additive attention as it performs a linear combination of encoder states and the decoder states."
      ],
      "metadata": {
        "id": "xED9oCtg2peG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is a Language Model?\n",
        "\n",
        "A Language Model ( LM ) captures the probability of a sequence of words in the language. Equivalently, it tells us how likely a given word will follow a sequence of words. Traditionally, N-gram models and their variants were used as language models"
      ],
      "metadata": {
        "id": "M8dvM_cu21tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is Multi-Head Attention?\n",
        "\n",
        "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel.Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies)."
      ],
      "metadata": {
        "id": "sUWKRQXl29Y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is Bilingual Evaluation Understudy (BLEU)\n",
        "\n",
        "BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations."
      ],
      "metadata": {
        "id": "2cxrpnkX3Gbd"
      }
    }
  ]
}