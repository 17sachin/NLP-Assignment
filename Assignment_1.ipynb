{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSPFDALuhz2NY+BX2C9Nwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/NLP-Assignment/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Explain One-Hot Encoding\n",
        "\n",
        "A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1."
      ],
      "metadata": {
        "id": "u3fLyvyhaX0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Explain Bag of Words.\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words.\n",
        "\n"
      ],
      "metadata": {
        "id": "W2u-gBd8bl7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Explain Bag of N-Grams\n",
        "\n",
        "A bag-of-n-grams model records the number of times that each n-gram appears in each document of a collection. An n-gram is a collection of n successive words. bagOfNgrams does not split text into words"
      ],
      "metadata": {
        "id": "LVOtHuVAbyLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Explain TF-IDF\n",
        "\n",
        "TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify words in a set of documents. We generally compute a score for each word to signify its importance in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining.\n",
        "\n"
      ],
      "metadata": {
        "id": "n8PVBbYtb6LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What is OOV problem?\n",
        "\n",
        "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. In speech recognition, it's the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning."
      ],
      "metadata": {
        "id": "cP0Tbzz0cJT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What are word embeddings?\n",
        "\n",
        "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems."
      ],
      "metadata": {
        "id": "e4X412MNcRWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Explain Continuous bag of words (CBOW)\n",
        "\n",
        "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle . While in the Skip-gram model, the distributed representation of the input word is used to predict the context"
      ],
      "metadata": {
        "id": "6xYvc0UzcYEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Explain SkipGram\n",
        "\n",
        "SkipGram is an algorithm that is used to create word embeddings i.e. high-dimensional vector representation of words. These embeddings are meant to encode the semantic meaning of words such that words that are semantically similar will lie close to each other in that vector's space"
      ],
      "metadata": {
        "id": "pCvpUpQgcmm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Explain Glove Embeddings.\n",
        "\n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space"
      ],
      "metadata": {
        "id": "lkQTmu5Dcvmj"
      }
    }
  ]
}