{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPflnz6KyWMWmQnyATIHsYP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/NLP-Assignment/blob/main/Assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Explain the architecture of BERT\n",
        "\n",
        "BERT is basically an Encoder stack of transformer architecture. A transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. BERTBASE has 12 layers in the Encoder stack while BERTLARGE has 24 layers in the Encoder stack"
      ],
      "metadata": {
        "id": "yPqIE2vr55WG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Explain Masked Language Modeling (MLM)\n",
        "\n",
        "MLM consists of giving BERT a sentence and optimizing the weights inside BERT to output the same sentence on the other side. So we input a sentence and ask that BERT outputs the same sentence. However, before we actually give BERT that input sentence â€” we mask a few tokens"
      ],
      "metadata": {
        "id": "lUuNETL_598C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Explain Next Sentence Prediction (NSP)\n",
        "\n",
        "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document"
      ],
      "metadata": {
        "id": "XWChd1MJ6FMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is Matthews evaluation?\n",
        "\n",
        "Matthew's correlation coefficient, also abbreviated as MCC was invented by Brian Matthews in 1975. MCC is a statistical tool used for model evaluation. Its job is to gauge or measure the difference between the predicted values and actual values and is equivalent to chi-square statistics for a 2 x 2 contingency table."
      ],
      "metadata": {
        "id": "eHw_knxA6WHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What is Matthews Correlation Coefficient (MCC)?\n",
        "\n",
        "The Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives)"
      ],
      "metadata": {
        "id": "SwWnbt-U6bU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain Semantic Role Labeling\n",
        "\n",
        "A semantic role is the underlying relationship that a participant has with the main verb in a clause. Discussion: Semantic role is the actual role a participant plays in some real or imagined situation, apart from the linguistic encoding of those situations."
      ],
      "metadata": {
        "id": "PV4eWDbZ6lL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Why Fine-tuning a BERT model takes less time than pretraining"
      ],
      "metadata": {
        "id": "rQE3NMjY6sPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Recognizing Textual Entailment (RTE)\n",
        "\n",
        "Textual entailment recognition is the task of deciding, given two text fragments, whether the meaning of one text is entailed (can be inferred) from another text (see the Instructions tab for the specific operational definition of textual entailment assumed in the challenge)."
      ],
      "metadata": {
        "id": "nI7uT8VX6-al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Explain the decoder stack of GPT models.\n",
        "\n",
        "GPT model was based on Transformer architecture. It was made of decoders stacked on top of each other (12 decoders). ... GPT model works on a principle called autoregressive which is similar to one used in RNN. It is a technique where the previous output becomes current input"
      ],
      "metadata": {
        "id": "S8UXntXe7Gc1"
      }
    }
  ]
}